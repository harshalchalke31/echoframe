{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Projects\\\\python\\\\echoframe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1) Class to split data into train/val/test\n",
    "# ----------------------------------------\n",
    "\n",
    "class DatasetSplitter:\n",
    "    \"\"\"\n",
    "    Splits raw dataset into ./data/train, ./data/val, ./data/test\n",
    "    according to FileList.csv. Each row in FileList.csv is assumed to have:\n",
    "        - 'FileName': e.g. '0X100009310A3BD7FC' (no .avi in the CSV)\n",
    "        - 'Split': 'train' or 'val' or 'test'\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 filelist_csv='./data/FileList.csv', \n",
    "                 source_dir='./data/raw_videos',\n",
    "                 target_dir='./data'):\n",
    "        \"\"\"\n",
    "        filelist_csv: path to FileList.csv\n",
    "        source_dir: directory where raw video files currently reside\n",
    "        target_dir: main directory where train/val/test subfolders will be created\n",
    "        \"\"\"\n",
    "        self.filelist_csv = filelist_csv\n",
    "        self.source_dir = source_dir\n",
    "        self.target_dir = target_dir\n",
    "\n",
    "        # Read CSV\n",
    "        if not os.path.exists(self.filelist_csv):\n",
    "            raise FileNotFoundError(f\"FileList CSV not found: {self.filelist_csv}\")\n",
    "        self.df = pd.read_csv(self.filelist_csv)\n",
    "\n",
    "        # Ensure mandatory columns exist\n",
    "        for col in ['FileName', 'Split']:\n",
    "            if col not in self.df.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in {self.filelist_csv}\")\n",
    "\n",
    "    def split_and_store(self):\n",
    "        \"\"\"Creates train/val/test folders and copies each file into the correct folder.\"\"\"\n",
    "        splits = ['train', 'val', 'test']\n",
    "        # Create subdirectories if they don't exist\n",
    "        for sp in splits:\n",
    "            split_dir = os.path.join(self.target_dir, sp)\n",
    "            if not os.path.exists(split_dir):\n",
    "                os.makedirs(split_dir)\n",
    "\n",
    "        # Go through each row in CSV, copy the file to the appropriate split folder\n",
    "        for idx, row in self.df.iterrows():\n",
    "            filename_raw = str(row['FileName'])  # e.g. '0X100009310A3BD7FC'\n",
    "            split = str(row['Split']).lower()\n",
    "            if split not in splits:\n",
    "                print(f\"Skipping file {filename_raw} with unknown split {split}\")\n",
    "                continue\n",
    "\n",
    "            # Ensure .avi extension in case the CSV has none\n",
    "            if not filename_raw.lower().endswith('.avi'):\n",
    "                filename_avi = filename_raw + '.avi'\n",
    "            else:\n",
    "                filename_avi = filename_raw\n",
    "\n",
    "            src_path = os.path.join(self.source_dir, filename_avi)\n",
    "            dst_path = os.path.join(self.target_dir, split, filename_avi)\n",
    "\n",
    "            if not os.path.exists(src_path):\n",
    "                print(f\"Source file not found: {src_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "        print(\"Data splitting & storing complete.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2) Dataset class for frames + binary mask\n",
    "# ----------------------------------------\n",
    "\n",
    "class EchoVolumeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads frames from videos stored under ./data/<split> \n",
    "    and builds a binary mask from VolumeTracings.csv for each frame.\n",
    "\n",
    "    NOTE:\n",
    "      - In FileList.csv, the 'FileName' may be without '.avi'.\n",
    "      - In VolumeTracings.csv, the 'FileName' includes '.avi'.\n",
    "      - After we split/copy, the actual video files in ./data/<split> will end with '.avi'.\n",
    "      - We unify everything by removing extensions (os.path.splitext) when building keys.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 split='train',\n",
    "                 data_dir='./data',\n",
    "                 volume_csv='./data/VolumeTracings.csv',\n",
    "                 resize=(112, 112),\n",
    "                 mean=(0.0, 0.0, 0.0),\n",
    "                 std=(1.0, 1.0, 1.0)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: 'train', 'val', or 'test'\n",
    "            data_dir: base directory holding subdirectories 'train', 'val', 'test'\n",
    "            volume_csv: path to VolumeTracings.csv\n",
    "            resize: tuple (height, width) for resizing frame/mask\n",
    "            mean, std: used to normalize frames\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        self.volume_csv = volume_csv\n",
    "        self.resize = resize\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "        # 1) Gather a list of .avi (or .mp4) under data_dir/split\n",
    "        self.video_dir = os.path.join(self.data_dir, self.split)\n",
    "        if not os.path.exists(self.video_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {self.video_dir}\")\n",
    "\n",
    "        self.video_files = sorted([\n",
    "            f for f in os.listdir(self.video_dir)\n",
    "            if f.lower().endswith('.avi') or f.lower().endswith('.mp4')\n",
    "        ])\n",
    "\n",
    "        # 2) Read the volume tracings CSV\n",
    "        if not os.path.exists(self.volume_csv):\n",
    "            raise FileNotFoundError(f\"VolumeTracings CSV not found: {self.volume_csv}\")\n",
    "        self.tracings_df = pd.read_csv(self.volume_csv)\n",
    "\n",
    "        # Build a dictionary of polygons keyed by (base_name, frame_idx)\n",
    "        self.polygons_dict = self._build_polygons_dict()\n",
    "\n",
    "        # Create an index mapping so each item is (video_file, frame_idx)\n",
    "        self.index_map = []\n",
    "        for vid in self.video_files:\n",
    "            video_path = os.path.join(self.video_dir, vid)\n",
    "            frame_count = self._get_num_frames(video_path)\n",
    "            for frame_idx in range(frame_count):\n",
    "                # Example: (\"0X100009310A3BD7FC.avi\", 0), ...\n",
    "                self.index_map.append((vid, frame_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name, frame_idx = self.index_map[idx]\n",
    "        video_path = os.path.join(self.video_dir, video_name)\n",
    "\n",
    "        # Load that single frame\n",
    "        frame = self._read_frame(video_path, frame_idx)\n",
    "        if frame is None:\n",
    "            frame = np.zeros((self.resize[0], self.resize[1], 3), dtype=np.float32)\n",
    "\n",
    "        # Create the mask for this frame\n",
    "        mask = self._create_mask(video_name, frame_idx, frame.shape[:2])\n",
    "\n",
    "        # Convert frame (H,W,3) -> (3,H,W), normalize\n",
    "        frame = frame.astype(np.float32) / 255.0\n",
    "        for c in range(3):\n",
    "            frame[:, :, c] = (frame[:, :, c] - self.mean[c]) / self.std[c]\n",
    "        frame = np.transpose(frame, (2,0,1))  # => (3, H, W)\n",
    "\n",
    "        # Mask => (1, H, W)\n",
    "        mask = np.expand_dims(mask, axis=0).astype(np.float32)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        frame_tensor = torch.from_numpy(frame)\n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "        return frame_tensor, mask_tensor\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Utility methods\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    def _build_polygons_dict(self):\n",
    "        \"\"\"\n",
    "        Parses VolumeTracings.csv to build a dict keyed by (base_name, frame_number),\n",
    "        each mapping to a list of polygons (each polygon is a list of (x,y) points).\n",
    "        This unifies .avi vs. no extension by always stripping the file extension.\n",
    "        \"\"\"\n",
    "        polygons_dict = {}\n",
    "\n",
    "        for _, row in self.tracings_df.iterrows():\n",
    "            raw_file = str(row['FileName'])    # e.g. '0X100009310A3BD7FC.avi'\n",
    "            frame_num = int(row['Frame'])      # e.g. 10\n",
    "            # Extract polygon coordinates (x1, y1, x2, y2, ...)\n",
    "            coords = []\n",
    "            cols = row.index.tolist()\n",
    "            # Skip the first 2 columns (FileName, Frame)\n",
    "            for col_idx in range(2, len(cols), 2):\n",
    "                x_col = cols[col_idx]\n",
    "                y_col = cols[col_idx+1]\n",
    "                if pd.isna(row[x_col]) or pd.isna(row[y_col]):\n",
    "                    break\n",
    "                x_val = float(row[x_col])\n",
    "                y_val = float(row[y_col])\n",
    "                coords.append((x_val, y_val))\n",
    "\n",
    "            # Remove extension so the key is consistent with the dataset video name\n",
    "            base_name = os.path.splitext(raw_file)[0]  # => '0X100009310A3BD7FC'\n",
    "            key = (base_name, frame_num)\n",
    "\n",
    "            if key not in polygons_dict:\n",
    "                polygons_dict[key] = []\n",
    "            polygons_dict[key].append(coords)\n",
    "\n",
    "        return polygons_dict\n",
    "\n",
    "    def _get_num_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return 0\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        return length\n",
    "\n",
    "    def _read_frame(self, video_path, frame_idx):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if not ret or frame is None:\n",
    "            return None\n",
    "\n",
    "        # Convert BGR -> RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Resize\n",
    "        frame = cv2.resize(frame, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        return frame\n",
    "\n",
    "    def _create_mask(self, video_name, frame_idx, hw_shape):\n",
    "        \"\"\"\n",
    "        Creates a binary mask using polygon fill. \n",
    "        'video_name' is something like '0X100009310A3BD7FC.avi',\n",
    "        so we strip the extension to get the dictionary key.\n",
    "        \"\"\"\n",
    "        base_name = os.path.splitext(video_name)[0]  # e.g. '0X100009310A3BD7FC'\n",
    "        key = (base_name, frame_idx)\n",
    "\n",
    "        mask = np.zeros(hw_shape, dtype=np.uint8)\n",
    "        if key not in self.polygons_dict:\n",
    "            return mask  # blank\n",
    "\n",
    "        # Fill each polygon\n",
    "        for polygon_coords in self.polygons_dict[key]:\n",
    "            pts = np.array(polygon_coords, dtype=np.int32).reshape((-1,1,2))\n",
    "            cv2.fillPoly(mask, [pts], 255)\n",
    "\n",
    "        # Convert 0..255 to 0..1\n",
    "        mask = (mask > 127).astype(np.uint8)\n",
    "        return mask\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3) Similar approach for test data\n",
    "# ----------------------------------------\n",
    "# If your test data also has filenames that differ in extension, \n",
    "# the code below still works because we unify them with os.path.splitext.\n",
    "\n",
    "class EchoVolumeTestDataset(EchoVolumeDataset):\n",
    "    def __init__(self, data_dir='./data', volume_csv='./data/VolumeTracings.csv',\n",
    "                 resize=(112,112), mean=(0,0,0), std=(1,1,1)):\n",
    "        super().__init__(\n",
    "            split='test',\n",
    "            data_dir=data_dir,\n",
    "            volume_csv=volume_csv,\n",
    "            resize=resize,\n",
    "            mean=mean,\n",
    "            std=std\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting & storing complete.\n",
      "Train dataset size: 1315340\n",
      "Frames batch shape: torch.Size([2, 3, 112, 112])\n",
      "Masks batch shape: torch.Size([2, 1, 112, 112])\n",
      "Test dataset size: 226460\n",
      "Test frame shape: torch.Size([1, 3, 112, 112])\n",
      "Test mask shape: torch.Size([1, 1, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 1) Split the dataset according to FileList.csv\n",
    "# splitter = DatasetSplitter(\n",
    "#     filelist_csv='.\\data\\EchoNet-Dynamic\\EchoNet-Dynamic\\FileList.csv',\n",
    "#     source_dir='.\\data\\EchoNet-Dynamic\\EchoNet-Dynamic\\Videos',  # location of original .avi or .mp4 files\n",
    "#     target_dir='./data'\n",
    "# )\n",
    "# splitter.split_and_store()\n",
    "\n",
    "# 2) Create a dataset for 'train'\n",
    "train_dataset = EchoVolumeDataset(\n",
    "    split='train',\n",
    "    data_dir='./data',\n",
    "    volume_csv='.\\data\\EchoNet-Dynamic\\EchoNet-Dynamic\\VolumeTracings.csv',\n",
    "    resize=(112, 112),\n",
    "    mean=(0.0, 0.0, 0.0),\n",
    "    std=(1.0, 1.0, 1.0)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "# Inspect a single batch\n",
    "for frames, masks in train_loader:\n",
    "    # frames shape: (B, 3, H, W)\n",
    "    # masks shape:  (B, 1, H, W)\n",
    "    print(\"Frames batch shape:\", frames.shape)\n",
    "    print(\"Masks batch shape:\", masks.shape)\n",
    "    break\n",
    "\n",
    "# 3) Test dataset\n",
    "test_dataset = EchoVolumeTestDataset(\n",
    "    data_dir='./data',\n",
    "    volume_csv='.\\data\\EchoNet-Dynamic\\EchoNet-Dynamic\\VolumeTracings.csv',\n",
    "    resize=(112, 112),\n",
    "    mean=(0.0, 0.0, 0.0),\n",
    "    std=(1.0, 1.0, 1.0)\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "\n",
    "# Example iteration over test data\n",
    "for frames, masks in test_loader:\n",
    "    # frames shape: (1, 3, H, W)\n",
    "    # masks shape:  (1, 1, H, W)\n",
    "    print(\"Test frame shape:\", frames.shape)\n",
    "    print(\"Test mask shape:\", masks.shape)\n",
    "    # Typically you'd run inference on these frames\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
